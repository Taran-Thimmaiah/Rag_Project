{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c0635-73eb-4879-ad95-e426386e3c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def fetch_rendered_html(url, wait_selector=None, wait_time=15):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    # Todo - Change the path to windows path\n",
    "    service = Service(r\"c:\\Users\\Nachappa\\chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        if wait_selector:\n",
    "            WebDriverWait(driver, wait_time).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, wait_selector))\n",
    "            )\n",
    "        rendered_html = driver.page_source\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return rendered_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bd86df2-824b-46e3-a292-edab96fffe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdownify\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class EmptyExtraction(Exception):\n",
    "    pass\n",
    "\n",
    "def remove_ui_blocks_general(text):\n",
    "    cleaned_lines = []\n",
    "    skip = False\n",
    "    \n",
    "    # keywords that usually indicate UI junk/login/footer\n",
    "    junk_keywords = [\n",
    "        'apply now', 'forgot password', 'request reset', \n",
    "         '√ó', 'loading', 'javascript',\n",
    "\n",
    "        # UI prompts\n",
    "        \"subscribe\", \"unsubscribe\", \"notification\", \"alert\", \"popup\",\n",
    "        \"scroll to\",\"faq\",\"send otp\", \"resend otp\", \"password\", \"create password\",\n",
    "        \"change password\", \"reset password\", \"confirm password\",\n",
    "        \"update your profile\", \"complete your profile\",\"sign in\", \"sign up\", \"logout\", \"otp\",\n",
    "        \"profile is currently under moderation\",\n",
    "        \"haven't received otp\", \"congratulations\", \"your password has been changed\",\n",
    "        \"thank you for subscribing\", \"don't have an account\",\n",
    "        \"character\",\"lowercase\",\"uppercase\", \"digit\",\"email id\",\"email address\",\n",
    "        \n",
    "\n",
    "        # Numbers / stats\n",
    "        \"users have visited\", \"last updated\", \"toll free\", \"working hrs\"\n",
    "\n",
    "    ]\n",
    "    \n",
    "    for line in text.splitlines():\n",
    "        stripped = line.strip().lower()\n",
    "        \n",
    "        # Skip lines that match UI patterns\n",
    "        if any(keyword in stripped for keyword in junk_keywords):\n",
    "            skip = True\n",
    "            continue\n",
    "        \n",
    "        # Stop skipping if line looks like main content (long enough, not just symbols)\n",
    "        if skip and len(stripped) > 15:\n",
    "            skip = False\n",
    "        \n",
    "        if not skip:\n",
    "            # Skip empty lines or lines with only symbols (*, +, -)\n",
    "            if re.match(r'^[\\*\\+\\-\\s]+$', stripped):\n",
    "                continue\n",
    "            cleaned_lines.append(line)\n",
    "            skip=True\n",
    "    \n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "def Clean_Markdown(url):\n",
    "    \n",
    "    try:\n",
    "        rendered_html = fetch_rendered_html(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching HTML from {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(rendered_html, \"html.parser\")\n",
    "        text_len = len(soup.get_text(strip=True))\n",
    "        if text_len < 50:  # threshold you decide\n",
    "            raise EmptyExtraction(f\"Extraction failed for {url}. HTML content is junk/empty.\")\n",
    "    except EmptyExtraction as e:\n",
    "        print(e)\n",
    "        return \"\"\n",
    "        \n",
    "        \n",
    "    \n",
    "    try:\n",
    "        markdown_string = markdownify.markdownify(rendered_html, heading_style='ATX')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to convert HTML to Markdown : {e}\") \n",
    "\n",
    "    \n",
    "    \n",
    "    # 1. Remove both markdown images (![...](...)) and links ([...](...))\n",
    "    \n",
    "    # Removes\n",
    "    # ![Seed Fund Need](/static/media/seedfund-need.png)\n",
    "    # ![](data:image/png;base64,abc123)\n",
    "    # [](/static/media/Launch.5a3470a4.mp4)\n",
    "    # [More Details](/about)\n",
    "    link_pattern = r\"\"\"\n",
    "        (?:!?\\[[^\\]]*\\]\\([^)]*\\))   # normal markdown image/link: [text](url) or ![alt](url)\n",
    "      | (?:\\]\\([^)]*\\))             # broken shorthand: ](url)\n",
    "    \"\"\"\n",
    "    removed_link = re.sub(link_pattern, \"\", markdown_string,flags=re.VERBOSE)\n",
    "    \n",
    "    \n",
    "    # 2. Remove Footer\n",
    "    footer_keywords = [\n",
    "            \"About\", \"Help\", \"Join\", \"Subscribe\", \"Follow\",\n",
    "            \"Terms of Use\", \"Privacy Policy\", \"Disclaimer\", \"Copyright\"\n",
    "        ]\n",
    "    \n",
    "    footer_pattern =r\"\\n###### (?:\" + \"|\".join(map(re.escape, footer_keywords)) + r\").*\"\n",
    "    \n",
    "    removed_footer = re.sub(footer_pattern , \"\", removed_link,flags=re.DOTALL)\n",
    "\n",
    "\n",
    "    # 3. Remove known UI junk blocks\n",
    "    ui_block_patterns = [\n",
    "        r\"(?s)please (?:enter|change).*?submit\",\n",
    "        r\"(?s)your password must.*?submit\",\n",
    "        r\"(?s)notification alert.*?(yes|no)\",\n",
    "        r\"(?s)do you really want to logout.*?(yes|no)\",\n",
    "    ]\n",
    "    for pat in ui_block_patterns:\n",
    "        removed_ui_junk = re.sub(pat, \"\", removed_footer, flags=re.IGNORECASE)\n",
    "    \n",
    "    \n",
    "    # 4. Removes Majority of Header, lines with only symbols (*, +, -) and junk words \n",
    "    removed_header_junk = remove_ui_blocks_general(removed_ui_junk)\n",
    "\n",
    "    # 5. Remove numeric/symbol junk lines\n",
    "    symbol_noise = [\n",
    "    r\"^\\s*¬©.*$\",                     # any line starting with ¬©\n",
    "    r\"^\\s*\\d{1,3}(?:[,\\d]+)*\\s*$\",   # pure numeric lines\n",
    "    r\"^\\s*\\d+\\s*/\\s*\\d+\\s*$\",        # pagination like 3/12\n",
    "    ]\n",
    "    removed_numeric_junk = removed_header_junk\n",
    "    for pat in symbol_noise:\n",
    "        removed_numeric_junk = re.sub(pat, \"\", removed_numeric_junk, flags=re.MULTILINE)\n",
    "    \n",
    "    \n",
    "    # 4. Normalize spaces and newlines for cleaner formatting.\n",
    "    lines = [line for line in removed_numeric_junk.splitlines() if line.strip()]\n",
    "    completely_cleaned = \"\\n\".join(lines)\n",
    "\n",
    "    display(set(markdown_string.splitlines()) - set(completely_cleaned.splitlines()))\n",
    "    return completely_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2e660-236a-4d4e-9cfb-0e13f758926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_markdown_with_retry(url, min_length=50, max_retries=5):\n",
    "    \"\"\"\n",
    "    Keep extracting markdown until it's not too short, \n",
    "    or until max_retries is reached.\n",
    "    \"\"\"\n",
    "    attempts = 0\n",
    "    markdown_content = \"\"\n",
    "\n",
    "    while attempts < max_retries:\n",
    "        markdown_content = Clean_Markdown(url)   # <- your function\n",
    "        if markdown_content and len(markdown_content.strip()) >= min_length:\n",
    "            return markdown_content  # ‚úÖ good content\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Attempt {attempts+1}: Content too small for {url}, retrying...\")\n",
    "            attempts += 1\n",
    "    \n",
    "    print(f\"‚ùå Failed to extract enough content from {url} after {max_retries} retries.\")\n",
    "    return markdown_content  # might still be small, but we return the last try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e46625-0296-4141-bc37-78f11d9fb024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType, utility\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def process_and_store_urls(urls, collection_name=\"startup_india_data\"):\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "        (\"#####\", \"Header 5\"),\n",
    "        (\"######\", \"Header 6\")\n",
    "    ]\n",
    "\n",
    "    sentence_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"!\", \"?\", \"* \"]\n",
    "    )\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # ---------------- Milvus Setup ----------------\n",
    "    connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "    if utility.has_collection(collection_name):\n",
    "        utility.drop_collection(collection_name)\n",
    "\n",
    "    fields = [\n",
    "        FieldSchema(name=\"chunk_id\", dtype=DataType.VARCHAR, is_primary=True, max_length=36),\n",
    "        FieldSchema(name=\"chunk_sequence\", dtype=DataType.INT64),\n",
    "        FieldSchema(name=\"chunk_text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "        FieldSchema(name=\"chunk_embed\", dtype=DataType.FLOAT_VECTOR, dim=384),\n",
    "        FieldSchema(name=\"doc_id\", dtype=DataType.VARCHAR, max_length=36),\n",
    "        FieldSchema(name=\"url\", dtype=DataType.VARCHAR, max_length=255),\n",
    "        FieldSchema(name=\"doc_timestamp\", dtype=DataType.VARCHAR, max_length=20)\n",
    "    ]\n",
    "    schema = CollectionSchema(fields, \"Startup India website scraped data\")\n",
    "    collection = Collection(collection_name, schema)\n",
    "\n",
    "    # ---------------- Processing Loop ----------------\n",
    "    all_combined_data = []\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Step 1: Fetch and clean markdown for URL\n",
    "            markdown_content = get_clean_markdown_with_retry(url, min_length=50, max_retries=5)\n",
    "            if not markdown_content or len(markdown_content.strip()) < 50:\n",
    "                 continue  # final fallback if all retries fail i.e skip it\n",
    "\n",
    "            # Step 2: Split with headers\n",
    "            header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "            header_docs = header_splitter.split_text(markdown_content)\n",
    "\n",
    "            doc_id = str(uuid.uuid4())\n",
    "            doc_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            chunks, metadatas = [], []\n",
    "\n",
    "            for i, doc in enumerate(header_docs):\n",
    "                sub_chunks = sentence_splitter.split_text(doc.page_content)\n",
    "                header = \"\\n\".join([f\"{k}: {v}\" for k, v in doc.metadata.items()]) if doc.metadata else \"\"\n",
    "                for sub_chunk in sub_chunks:\n",
    "                    chunk_text = f\"{header}\\n{sub_chunk}\".strip() if header else sub_chunk\n",
    "                    chunks.append(chunk_text)\n",
    "                    metadatas.append({\n",
    "                        \"chunk_id\": str(uuid.uuid4()),\n",
    "                        \"chunk_sequence\": i,\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"url\": url,\n",
    "                        \"doc_timestamp\": doc_timestamp\n",
    "                    })\n",
    "\n",
    "            if not chunks:\n",
    "                print(f\"‚ö†Ô∏è No chunks extracted for {url}\")\n",
    "                continue\n",
    "\n",
    "            # Step 3: Generate embeddings\n",
    "            chunk_embeddings = embeddings.embed_documents(chunks)\n",
    "\n",
    "            # Step 4: Combine data\n",
    "            combined_data = [\n",
    "                {\n",
    "                    \"chunk_id\": meta[\"chunk_id\"],\n",
    "                    \"chunk_sequence\": meta[\"chunk_sequence\"],\n",
    "                    \"chunk_text\": chunk,\n",
    "                    \"chunk_embed\": embedding,\n",
    "                    \"doc_id\": meta[\"doc_id\"],\n",
    "                    \"url\": meta[\"url\"],\n",
    "                    \"doc_timestamp\": meta[\"doc_timestamp\"]\n",
    "                }\n",
    "                for chunk, embedding, meta in zip(chunks, chunk_embeddings, metadatas)\n",
    "            ]\n",
    "\n",
    "            all_combined_data.extend(combined_data)\n",
    "            print(f\"‚úÖ Processed {url} ({len(chunks)} chunks)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {url}: {e}\")\n",
    "\n",
    "    # ---------------- Insert into Milvus ----------------\n",
    "    if all_combined_data:\n",
    "        collection.insert(all_combined_data)\n",
    "        collection.create_index(\n",
    "            field_name=\"chunk_embed\",\n",
    "            index_params={\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 128}}\n",
    "        )\n",
    "        collection.create_index(field_name=\"doc_id\", index_params={\"index_type\": \"TRIE\"})\n",
    "        collection.load()\n",
    "        print(f\"üéâ Inserted {len(all_combined_data)} chunks from {len(urls)} docs into {collection_name}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No data to insert.\")\n",
    "\n",
    "\n",
    "# ---------------- Example Usage ----------------\n",
    "urls = [\n",
    "    \"https://www.startupindia.gov.in/\",\n",
    "    \"https://seedfund.startupindia.gov.in/\",\n",
    "    \"https://www.sidbivcf.in/en/funds/ffs\",\n",
    "]\n",
    "\n",
    "process_and_store_urls(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848810f9-092a-4c83-9674-26bf400501fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType, utility\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Assume Clean_Markdown(url) is defined elsewhere and returns cleaned Markdown string\n",
    "# Example: def Clean_Markdown(url): ... return markdown_content\n",
    "\n",
    "def get_clean_markdown_with_retry(url, min_length=10, max_retries=5, retry_delay=2):\n",
    "    \"\"\"Retry Clean_Markdown(url) with specified retries and validate content length.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            markdown_content = Clean_Markdown(url)\n",
    "            if markdown_content and len(markdown_content.strip()) >= min_length:\n",
    "                return markdown_content\n",
    "            print(f\"‚ö†Ô∏è Content too short for {url} (length: {len(markdown_content.strip() if markdown_content else '')}), retrying...\")\n",
    "        except (requests.RequestException, ValueError) as e:\n",
    "            print(f\"‚ö†Ô∏è Attempt {attempt + 1}/{max_retries} failed for {url}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "    return \"\"  # Return empty string if all retries fail\n",
    "\n",
    "def process_and_store_urls(urls, collection_name=\"startup_india_data\"):\n",
    "    \"\"\"Process URLs, chunk Markdown content, generate embeddings, and store in Milvus.\"\"\"\n",
    "    # Define splitters and embeddings\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "        (\"#####\", \"Header 5\"),\n",
    "        (\"######\", \"Header 6\")\n",
    "    ]\n",
    "    sentence_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # Increased for longer paragraphs\n",
    "        chunk_overlap=50,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"!\", \"?\", \"* \"]\n",
    "    )\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Milvus setup\n",
    "    try:\n",
    "        connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "        print(\"Connected to Milvus server.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to connect to Milvus: {e}\")\n",
    "        return\n",
    "\n",
    "    # Create or recreate collection\n",
    "    if utility.has_collection(collection_name):\n",
    "        utility.drop_collection(collection_name)\n",
    "    \n",
    "    fields = [\n",
    "        FieldSchema(name=\"chunk_id\", dtype=DataType.VARCHAR, is_primary=True, max_length=36),\n",
    "        FieldSchema(name=\"chunk_sequence\", dtype=DataType.INT64),\n",
    "        FieldSchema(name=\"chunk_text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "        FieldSchema(name=\"chunk_embed\", dtype=DataType.FLOAT_VECTOR, dim=384),\n",
    "        FieldSchema(name=\"doc_id\", dtype=DataType.VARCHAR, max_length=36),\n",
    "        FieldSchema(name=\"url\", dtype=DataType.VARCHAR, max_length=255),\n",
    "        FieldSchema(name=\"doc_timestamp\", dtype=DataType.VARCHAR, max_length=20)\n",
    "    ]\n",
    "    schema = CollectionSchema(fields, \"Startup India website scraped data\")\n",
    "    collection = Collection(collection_name, schema)\n",
    "\n",
    "    # Process URLs\n",
    "    total_chunks = 0\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Fetch and clean markdown\n",
    "            markdown_content = get_clean_markdown_with_retry(url, min_length=10, max_retries=5)\n",
    "            if not markdown_content or len(markdown_content.strip()) < 10:\n",
    "                print(f\"‚ö†Ô∏è Skipping {url}: No valid content (length: {len(markdown_content.strip() if markdown_content else '')})\")\n",
    "                continue\n",
    "\n",
    "            # Split with headers\n",
    "            header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "            header_docs = header_splitter.split_text(markdown_content)\n",
    "            doc_id = str(uuid.uuid4())  # Unique doc_id per URL\n",
    "            doc_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            chunks, metadatas = [], []\n",
    "\n",
    "            # Split content into sub-chunks\n",
    "            for i, doc in enumerate(header_docs):\n",
    "                sub_chunks = sentence_splitter.split_text(doc.page_content)\n",
    "                header = \"\\n\".join([f\"{k}: {v}\" for k, v in doc.metadata.items()]) if doc.metadata else \"\"\n",
    "                for sub_chunk in sub_chunks:\n",
    "                    chunk_text = f\"{header}\\n{sub_chunk}\".strip() if header else sub_chunk\n",
    "                    chunks.append(chunk_text)\n",
    "                    metadatas.append({\n",
    "                        \"chunk_id\": str(uuid.uuid4()),\n",
    "                        \"chunk_sequence\": i,\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"url\": url,\n",
    "                        \"doc_timestamp\": doc_timestamp\n",
    "                    })\n",
    "\n",
    "            if not chunks:\n",
    "                print(f\"‚ö†Ô∏è No chunks extracted for {url}\")\n",
    "                continue\n",
    "\n",
    "            # Generate embeddings\n",
    "            chunk_embeddings = embeddings.embed_documents(chunks)\n",
    "\n",
    "            # Combine data\n",
    "            combined_data = [\n",
    "                {\n",
    "                    \"chunk_id\": meta[\"chunk_id\"],\n",
    "                    \"chunk_sequence\": meta[\"chunk_sequence\"],\n",
    "                    \"chunk_text\": chunk,\n",
    "                    \"chunk_embed\": embedding,\n",
    "                    \"doc_id\": meta[\"doc_id\"],\n",
    "                    \"url\": meta[\"url\"],\n",
    "                    \"doc_timestamp\": meta[\"doc_timestamp\"]\n",
    "                }\n",
    "                for chunk, embedding, meta in zip(chunks, chunk_embeddings, metadatas)\n",
    "            ]\n",
    "\n",
    "            # Insert per URL to manage memory\n",
    "            collection.insert(combined_data)\n",
    "            total_chunks += len(chunks)\n",
    "            print(f\"‚úÖ Processed {url} ({len(chunks)} chunks, doc_id: {doc_id})\")\n",
    "\n",
    "        except (requests.RequestException, ValueError) as e:\n",
    "            print(f\"‚ùå Error processing {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Create indexes and load collection\n",
    "    if total_chunks > 0:\n",
    "        collection.create_index(\n",
    "            field_name=\"chunk_embed\",\n",
    "            index_params={\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 128}}\n",
    "        )\n",
    "        collection.create_index(field_name=\"doc_id\", index_params={\"index_type\": \"TRIE\"})\n",
    "        collection.load()\n",
    "        print(f\"üéâ Inserted {total_chunks} chunks from {len(urls)} URLs into {collection_name}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No data inserted into collection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca1111-9b89-492c-9d63-85bb003e7f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "        \"https://www.startupindia.gov.in/content/sih/en/home-page.html\",\n",
    "        \"https://www.startupindia.gov.in/content/sih/en/startup.html\",\n",
    "        \"https://www.startupindia.gov.in/content/sih/en/schemes.html\"\n",
    "    ]\n",
    "process_and_store_urls(urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
